{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Embedding,Dense\n",
    "from keras.layers import LSTM, Input, merge, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"glove.6B.100d.txt\"\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    \n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "       content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400001  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model= loadGloveModel(file)   \n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser= WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "questions=[]\n",
    "answers=[]\n",
    "answer=[]\n",
    "length=30\n",
    "sentence_length=length\n",
    "with open(\"sample1.txt\",\"r\") as f:\n",
    "    choice=0\n",
    "    for line in f:\n",
    "        line=line.lower()\n",
    "        sentence=line.translate(string.punctuation)\n",
    "        sentence=line[2:].split()\n",
    "        word_vec=[]\n",
    "        for word in sentence:\n",
    "            if word in model:\n",
    "                word_vec.append(model[word])\n",
    "            else:\n",
    "                stemmed_word=stemmer.stem(word)\n",
    "                lemmatised_word=lemmatiser.lemmatize(word)\n",
    "                if stemmed_word in model:\n",
    "                    word_vec.append(model[stemmed_word])\n",
    "                elif lemmatised_word in model:\n",
    "                    word_vec.append(model[lemmatised_word])        \n",
    "        if choice==0:\n",
    "            choice=1\n",
    "            if len(word_vec)<=sentence_length:\n",
    "                for vec_len in range(length-len(word_vec)):\n",
    "                    word_vec.append(model[\"0\"])\n",
    "            questions.append(word_vec[:length])\n",
    "            \n",
    "        else:\n",
    "            choice=0\n",
    "            if len(word_vec)<=sentence_length:\n",
    "                for vec_len in range(length-len(word_vec)):\n",
    "                    word_vec.append(model[\"0\"])\n",
    "                answers.append(word_vec)\n",
    "                answer.append(line)\n",
    "            else:\n",
    "                del questions[-1]\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vec_len=100\n",
    "questions=np.asarray(questions)\n",
    "answers=np.asarray(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions,answers=shuffle(questions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(questions, answers, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_y_train=[y_train[random.randint(0,len(y_train)-1)] for i in range(len(y_train)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_y_train=np.asarray(bad_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((906, 30, 100), (48, 30, 100))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, good_answers, bad_answers=X_train,y_train,bad_y_train\n",
    "Y = np.zeros(shape=(questions.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(906, 30, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def loss_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "from keras.layers import Embedding,Dense,Flatten,Activation,RepeatVector,Permute,multiply\n",
    "\n",
    "margin = 0.2\n",
    "enc_timesteps = 60\n",
    "dec_timesteps = 60\n",
    "hidden_dim = 15\n",
    "epoch=10\n",
    "maxlen=60\n",
    "# initialize the question and answer shapes and datatype\n",
    "question = Input(shape=(length,100))\n",
    "answer = Input(shape=(length,100))\n",
    "answer_good = Input(shape=(length,100))\n",
    "answer_bad = Input(shape=(length,100))\n",
    "\n",
    "q_bilstm=Bidirectional(LSTM(activation='tanh', units=hidden_dim, return_sequences=True))(question)\n",
    "q_atten=attention = Dense(1, activation='tanh')(q_bilstm)\n",
    "q_atten = Flatten()(q_atten)\n",
    "q_atten = Activation('softmax')(q_atten)\n",
    "q_atten = RepeatVector(length)(q_atten)\n",
    "q_atten = Permute([2, 1])(q_atten)\n",
    "q_sent_representation = Lambda(lambda xin: K.sum(xin[0]*xin[1], axis=-2), output_shape=(length,))([q_bilstm, q_atten])\n",
    "\n",
    "a_bilstm=Bidirectional(LSTM(activation='tanh', units=hidden_dim, return_sequences=True))(answer)\n",
    "a_atten=attention = Dense(1, activation='tanh')(a_bilstm)\n",
    "a_atten = Flatten()(a_atten)\n",
    "a_atten = Activation('softmax')(a_atten)\n",
    "a_atten = RepeatVector(length)(a_atten)\n",
    "a_atten = Permute([2, 1])(a_atten)\n",
    "a_sent_representation = Lambda(lambda xin: K.sum(xin[0]*xin[1], axis=-2), output_shape=(length,))([a_bilstm, a_atten])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question_enc_1 = q_sent_representation\n",
    "\n",
    "answer_enc_1 = a_sent_representation\n",
    "\n",
    "distance = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([question_enc_1, answer_enc_1])\n",
    "\n",
    "lstm_model = Model(inputs=[question, answer], outputs=[distance])\n",
    "good_similarity = lstm_model([question, answer_good])\n",
    "\n",
    "bad_similarity = lstm_model([question, answer_bad])\n",
    "\n",
    "loss=Lambda(lambda x: K.relu(margin - x[0] + x[1]),output_shape=lambda x: x[0])([good_similarity, bad_similarity])\n",
    "\n",
    "\n",
    "# return training and prediction model\n",
    "training_model = Model(inputs=[question, answer_good, answer_bad], outputs=loss, name='training_model')\n",
    "training_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=\"rmsprop\")\n",
    "prediction_model = Model(inputs=[question, answer_good], outputs=good_similarity, name='prediction_model')\n",
    "prediction_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1676 - val_loss: 0.1811\n",
      "1\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1688 - val_loss: 0.1784\n",
      "2\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 8ms/step - loss: 0.1691 - val_loss: 0.1780\n",
      "3\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1680 - val_loss: 0.1751\n",
      "4\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1679 - val_loss: 0.1732\n",
      "5\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1701 - val_loss: 0.1802\n",
      "6\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1709 - val_loss: 0.1768\n",
      "7\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1703 - val_loss: 0.1817\n",
      "8\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1690 - val_loss: 0.1784\n",
      "9\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1686 - val_loss: 0.1719\n",
      "10\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1697 - val_loss: 0.1745\n",
      "11\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1699 - val_loss: 0.1744\n",
      "12\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1687 - val_loss: 0.1798\n",
      "13\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1666 - val_loss: 0.1772\n",
      "14\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1692 - val_loss: 0.1726\n",
      "15\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1685 - val_loss: 0.1727\n",
      "16\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1671 - val_loss: 0.1790\n",
      "17\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1669 - val_loss: 0.1814\n",
      "18\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 8ms/step - loss: 0.1669 - val_loss: 0.1747\n",
      "19\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1688 - val_loss: 0.1751\n",
      "20\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1673 - val_loss: 0.1731\n",
      "21\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1663 - val_loss: 0.1719\n",
      "22\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1693 - val_loss: 0.1735\n",
      "23\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1681 - val_loss: 0.1769\n",
      "24\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1679 - val_loss: 0.1766\n",
      "25\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1711 - val_loss: 0.1753\n",
      "26\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1658 - val_loss: 0.1769\n",
      "27\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1679 - val_loss: 0.1802\n",
      "28\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1702 - val_loss: 0.1780\n",
      "29\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1688 - val_loss: 0.1708\n",
      "30\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1669 - val_loss: 0.1737\n",
      "31\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1686 - val_loss: 0.1707\n",
      "32\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1670 - val_loss: 0.1734\n",
      "33\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1677 - val_loss: 0.1772\n",
      "34\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1691 - val_loss: 0.1735\n",
      "35\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1690 - val_loss: 0.1774\n",
      "36\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1687 - val_loss: 0.1768\n",
      "37\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1680 - val_loss: 0.1767\n",
      "38\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1681 - val_loss: 0.1772\n",
      "39\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1674 - val_loss: 0.1745\n",
      "40\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1691 - val_loss: 0.1746\n",
      "41\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1687 - val_loss: 0.1785\n",
      "42\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1664 - val_loss: 0.1785\n",
      "43\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1669 - val_loss: 0.1713\n",
      "44\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1703 - val_loss: 0.1701\n",
      "45\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1692 - val_loss: 0.1788\n",
      "46\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1669 - val_loss: 0.1715\n",
      "47\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1670 - val_loss: 0.1705\n",
      "48\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1660 - val_loss: 0.1770\n",
      "49\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1677 - val_loss: 0.1762\n",
      "50\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1681 - val_loss: 0.1723\n",
      "51\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1684 - val_loss: 0.1739\n",
      "52\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1662 - val_loss: 0.1748\n",
      "53\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1678 - val_loss: 0.1803\n",
      "54\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 8ms/step - loss: 0.1686 - val_loss: 0.1712\n",
      "55\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1704 - val_loss: 0.1762\n",
      "56\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1685 - val_loss: 0.1702\n",
      "57\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1699 - val_loss: 0.1820\n",
      "58\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1677 - val_loss: 0.1765\n",
      "59\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1678 - val_loss: 0.1751\n",
      "60\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1675 - val_loss: 0.1772\n",
      "61\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1694 - val_loss: 0.1829\n",
      "62\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1692 - val_loss: 0.1798\n",
      "63\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1671 - val_loss: 0.1815\n",
      "64\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1679 - val_loss: 0.1744\n",
      "65\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1656 - val_loss: 0.1717\n",
      "66\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1683 - val_loss: 0.1781\n",
      "67\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1692 - val_loss: 0.1804\n",
      "68\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1665 - val_loss: 0.1802\n",
      "69\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1688 - val_loss: 0.1791\n",
      "70\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1677 - val_loss: 0.1785\n",
      "71\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1718 - val_loss: 0.1903\n",
      "72\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1766 - val_loss: 0.1760\n",
      "73\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1663 - val_loss: 0.1787\n",
      "74\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1711 - val_loss: 0.1762\n",
      "75\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1733 - val_loss: 0.1753\n",
      "76\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1707 - val_loss: 0.1707\n",
      "77\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1693 - val_loss: 0.1746\n",
      "78\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1676 - val_loss: 0.1803\n",
      "79\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1691 - val_loss: 0.1729\n",
      "80\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1675 - val_loss: 0.1727\n",
      "81\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1700 - val_loss: 0.1774\n",
      "82\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1703 - val_loss: 0.1668\n",
      "83\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1674 - val_loss: 0.1750\n",
      "84\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1672 - val_loss: 0.1751\n",
      "85\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1683 - val_loss: 0.1772\n",
      "86\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1687 - val_loss: 0.1803\n",
      "87\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1708 - val_loss: 0.1770\n",
      "88\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1674 - val_loss: 0.1732\n",
      "89\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1690 - val_loss: 0.1736\n",
      "90\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1676 - val_loss: 0.1710\n",
      "91\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1672 - val_loss: 0.1740\n",
      "92\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1679 - val_loss: 0.1711\n",
      "93\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1684 - val_loss: 0.1787\n",
      "94\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1672 - val_loss: 0.1757\n",
      "95\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1681 - val_loss: 0.1775\n",
      "96\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1688 - val_loss: 0.1739\n",
      "97\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1677 - val_loss: 0.1762\n",
      "98\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1694 - val_loss: 0.1740\n",
      "99\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1693 - val_loss: 0.1742\n",
      "100\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1676 - val_loss: 0.1713\n",
      "101\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1673 - val_loss: 0.1751\n",
      "102\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1687 - val_loss: 0.1719\n",
      "103\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1679 - val_loss: 0.1776\n",
      "104\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1682 - val_loss: 0.1780\n",
      "105\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1679 - val_loss: 0.1788\n",
      "106\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1681 - val_loss: 0.1757\n",
      "107\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1676 - val_loss: 0.1701\n",
      "108\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1672 - val_loss: 0.1712\n",
      "109\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1687 - val_loss: 0.1729\n",
      "110\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1698 - val_loss: 0.1809\n",
      "111\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1663 - val_loss: 0.1713\n",
      "112\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1671 - val_loss: 0.1800\n",
      "113\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 7ms/step - loss: 0.1674 - val_loss: 0.1766\n",
      "114\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1672 - val_loss: 0.1794\n",
      "115\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 5s 6ms/step - loss: 0.1671 - val_loss: 0.1764\n",
      "116\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 6s 7ms/step - loss: 0.1688 - val_loss: 0.1763\n",
      "117\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 7s 9ms/step - loss: 0.1680 - val_loss: 0.1719\n",
      "118\n",
      "Train on 815 samples, validate on 91 samples\n",
      "Epoch 1/1\n",
      "520/815 [==================>...........] - ETA: 1s - loss: 0.1665"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e7205c759994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/shinnu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    bad_answers=[y_train[random.randint(0,len(y_train)-1)] for i in range(len(y_train)) ]\n",
    "    bad_answers=np.asarray(bad_answers)\n",
    "    print(i)\n",
    "    training_model.fit(\n",
    "                [questions, good_answers, bad_answers],\n",
    "                Y,\n",
    "                epochs=1,\n",
    "                batch_size=20,\n",
    "                validation_split=0.1,\n",
    "                verbose=1\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=0\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "#     print(\"################################\",i)\n",
    "    answers=y_test\n",
    "    question=[X_test[i]]*len(X_test)\n",
    "\n",
    "    sims = prediction_model.predict([question, answers])\n",
    "    sims=list(sims)\n",
    "\n",
    "    a=sorted(np.unique(sims),reverse=True)\n",
    "    element=sims[i]\n",
    "    acc+=1/(a.index(element)+1)\n",
    "\n",
    "#     print(acc)\n",
    "print(\"MRR\",acc/len(X_test))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "prediction_model.save('Self-attention network model-c.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "# model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shinnu",
   "language": "python",
   "name": "shinnu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
